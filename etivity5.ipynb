{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Regression and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise repeats the same scenario as Lab 4, but this time we train a regression model, i.e. a model for numeric prediction. We also add dimensionality reduction element to the training pipeline. Note that dimensionality reduction can be included in classification pipelines in the same way. \n",
    "\n",
    "In this exercise we use the `winequality-red` dataset, taken from https://archive.ics.uci.edu/ml/datasets/wine+quality. Each example in this dataset represents a particular red wine. The columns (i.e. the features) are numerical characteristics of the wines. One of them is the `quality` of the wine on a scale from 3 to 8, and all others are chemical characteristics of the wine. \n",
    "\n",
    "We aim which at training a regression model for predicting the quality of a red wine. We will evaluate two regression algorithms, `RandomForestRegressor` and `LinearRegression`, to choose the better one for training the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# imports necessary for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import svm\n",
    "\n",
    "# regression algorithms\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# metrics for evaluating regression models\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a brief EDA to check for missing values and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./winequality-white.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4898.000000\n",
       "mean        5.877909\n",
       "std         0.885639\n",
       "min         3.000000\n",
       "25%         5.000000\n",
       "50%         6.000000\n",
       "75%         6.000000\n",
       "max         9.000000\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quality'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's check for missing values and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values. Let's check the boxplots of all columns except `quality` for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot(figsize=(20,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns `free sulfur dioxide` and `total sulfur dioxide` appear to have most prominent outliers and `residual sugar` also have outliers.  We will apply log transformation followed by `RobustScaler` to those 3 columns. To all other columns (except `quality`) we will apply `StandardScaler`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to prepare two lists of column names. The list `names_outliers` contains the names of the two columns to which we will apply log transformation followed by `RobustScaler`. The list `names_no_outliers` contains the names of all other columns (except `quality`) to which we will apply `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store a list with the names of all predictors\n",
    "names_all = [c for c in df if c not in ['quality']]\n",
    "\n",
    "# define column groups with the same data preparation\n",
    "names_outliers = ['residual sugar','free sulfur dioxide', 'total sulfur dioxide']\n",
    "names_no_outliers = list(set(names_all) - set(names_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting a dataset into a training and test sets, the names of the columns are lost. This is the reason, we stored the names of the columns in lists above. We will use the following class in the preprocessing pipeline to put the names of the columns back. We need this to easily apply the different preparation strategies to the two groups of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddColumnNames(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(data=X, columns=self.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need another class to be able to select a particular group of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        return X[self.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can build the preprocessing pipeline. It first adds the column names back to a set of examples (that can be either a training, or a validation, or test set). Then it applies the two different data preparation strategies to the two groups of columns and unites them with `FeatureUnion`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = make_pipeline(\n",
    "    AddColumnNames(columns=names_all),\n",
    "    FeatureUnion(transformer_list=[\n",
    "        (\"outlier_columns\", make_pipeline(\n",
    "            ColumnSelector(columns=names_outliers),\n",
    "            FunctionTransformer(np.log, validate=True),\n",
    "            RobustScaler()\n",
    "        )),\n",
    "        (\"no_outlier_columns\", make_pipeline(\n",
    "            ColumnSelector(columns=names_no_outliers),\n",
    "            StandardScaler()\n",
    "        ))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can separate the columns into *target* and *predictors* and split the dataset into a training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['quality']\n",
    "X = df.drop('quality', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we treat `quality` as a numerical attribute, it is in fact ordinal, and we can still do a stratified split to ensure that the distribution of wine qualities is the same in both the training and the test sets.\n",
    "\n",
    "Note that after the split into a training and test sets, X_train and X_test are numpy arrays and no longer have column names. That's why we needed the class above to put the names of columns back in the preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Best Parameters and Best Dimensionality Reduction Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train `RandomForestRegressor` on the training set with a range of possible parameters in order to find the best parameters by cross-validation. To do this we will build another [main] pipeline which includes the preprocessing pipeline and `RandomForestRegressor`. We also add an element for dimensionality reduction after the preprocessing pipeline.\n",
    "\n",
    "Here we will attempt three different dimensionality reduction methods and we will let the grid search pick the best one. These are:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Recursive Feature Elimination (RFE) with estimator `svm.SVR`\n",
    "- Recursive Feature Elimination (RFE) with estimator `LinearRegression`\n",
    "\n",
    "Note that RFE is using regression algorithms for selecting the best features. These regression algorithms can be different from the regression algorithm at the end of the main pipeline.\n",
    "\n",
    "The main pipeline will take care for separately preprocessing the training and validation sets after the training set is further split into training and validation sets in the process of cross-validation. It also applies the dimensionality reduction method separately to the two sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline contains a placeholder for the dimensionality reduction method. We will treat the method as a parameter and let the grid search pick the best of the three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('preprocess', preprocess_pipeline), \n",
    "                       ('reduce_dim', 'passthrough'),\n",
    "                       ('regresson', RandomForestRegressor(n_estimators=10))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the parameter grid to a few options for the `max_depth` parameter of `RandomForestRegressor` and to three alternative values for the number of selected features by the dimensionality reduction method. More parameters and values can be explored. Here we limit the options to make sure the grid search does not take too long to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES_OPTIONS = [2, 6, 11]\n",
    "MAX_DEPTH_OPTIONS = [2, 4, 6, 8]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(iterated_power=7)],\n",
    "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'regresson__max_depth': MAX_DEPTH_OPTIONS\n",
    "    },\n",
    "    {\n",
    "        'reduce_dim': [RFE(svm.SVR(kernel='linear', gamma='auto')),RFE(RandomForestRegressor(n_estimators=10))],\n",
    "        'reduce_dim__n_features_to_select': N_FEATURES_OPTIONS,\n",
    "        'regresson__max_depth': MAX_DEPTH_OPTIONS\n",
    "    }  \n",
    "]\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, cv=10, iid = False, refit=True)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV score = %0.3f\" % search.best_score_)\n",
    "print(\"Best parameters: \", search.best_params_)\n",
    "\n",
    "# store the best params and best model for later use\n",
    "RF_best_params = search.best_params_\n",
    "RF_best_model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explicitly assign the `False` to the parameter `iid` of `GridSearchCV` to avoid a deprecation warning. The parameter `refit=True` makes the `GridSearchCV` train a `RandomForestRegressor` model on the **whole training set** with the best parameters and the best dimensionality reduction method found. This best model can then be accessed via the `.best_estimator_` attribute of the `GridSearchCV`.\n",
    "\n",
    "Let's repeat the same experiment but with `LinearRegression` for training a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('preprocess', preprocess_pipeline), \n",
    "                       ('reduce_dim', 'passthrough'),\n",
    "                       ('regresson', LinearRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we limit the parameter grid to one parameter of `LinearRegression` and three alternative values for the number of selected features to make sure the grid search does not take too long to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES_OPTIONS = [2, 6, 11]\n",
    "NORMALIZE_OPTIONS = [False, True]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(iterated_power=7)],\n",
    "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'regresson__normalize': NORMALIZE_OPTIONS\n",
    "    },\n",
    "    {\n",
    "        'reduce_dim': [RFE(svm.SVR(kernel='linear', gamma='auto')),RFE(LinearRegression())],\n",
    "        'reduce_dim__n_features_to_select': N_FEATURES_OPTIONS,\n",
    "        'regresson__normalize': NORMALIZE_OPTIONS\n",
    "    }  \n",
    "]\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=10, iid=False, refit=True)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best CV score = %0.3f\" % search.best_score_)\n",
    "print(\"Best parameters: \", search.best_params_)\n",
    "\n",
    "# store the best params and best model for later use\n",
    "LR_best_params = search.best_params_\n",
    "LR_best_model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results suggest that Random Forest performs better. The cross-validation score is `1 - relative squared error`. The higher the score the more accurate the model. We can now further confirm this by comparing the best models on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Regression Models on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the best models found by the grid search on the test dataset and compare their metrics:\n",
    "\n",
    "- mean squared error (MSE)\n",
    "- mean absolute error (MAE)\n",
    "- 1-relative squared error (R2)\n",
    "\n",
    "to choose the better regressor for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model - a trained regression model\n",
    "\n",
    "def evaluate_model(X_test, y_test, model):\n",
    "    \n",
    "    # compute predictiond for the test set\n",
    "    _predicted_values = model.predict(X_test)\n",
    "        \n",
    "    # compute metrics\n",
    "    _mse = mean_squared_error(y_test, _predicted_values)\n",
    "    _mae = mean_absolute_error(y_test, _predicted_values)\n",
    "    _r2 = r2_score(y_test, _predicted_values)\n",
    "            \n",
    "    return _mse, _mae, _r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the function above to evaluate the best Random Forest and Linear Regression models found by the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_mse, RF_mae, RF_r2 = evaluate_model(X_test, y_test, RF_best_model)\n",
    "LR_mse, LR_mae, LR_r2 = evaluate_model(X_test, y_test, LR_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a Pandas bar plot to compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_metrics = np.array([RF_mse, RF_mae, RF_r2])\n",
    "LR_metrics = np.array([LR_mse, LR_mae, LR_r2])\n",
    "index = ['MSE', 'MAE', 'R2']\n",
    "df_metrics = pd.DataFrame({'Random Forest': RF_metrics, 'Linear Regression': LR_metrics}, index=index)\n",
    "df_metrics.plot.bar(rot=0)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tests confirms that Random Forest is the better regression model with lower MSE and MAE and higher R2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train a Random Forest regression model with all the data we have, assuming that the more data we have the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove the string 'regresson__' from the names of the best parameters\n",
    "def transform(dict, prefix):\n",
    "    dict_prefix = {key:value for key,value in dict.items() if prefix in key}\n",
    "    return {key.replace(prefix,''):value for key,value in dict_prefix.items()}\n",
    "\n",
    "pipe = make_pipeline(preprocess_pipeline, \n",
    "                     RF_best_params.get('reduce_dim'),\n",
    "                     RandomForestRegressor(n_estimators=10, **transform(RF_best_params, 'regresson__')))\n",
    "\n",
    "final_model =pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also store this model on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'final_model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
